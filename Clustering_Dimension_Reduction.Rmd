---
title: "Unsupervised learning - clustering and dimension reduction"
author: "Anna Yeaton"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = T)
```

# Lab Section
Download auto data from the *Statistical Learning* book website here: http://www-bcf.usc.edu/~gareth/ISL/data.html

Today, we are going over Hierarchical clustering, K-Means Clustering, PCA, and ICA. 

```{r load, include=FALSE}
library(ggplot2)
library(tidyverse)
library(ggfortify)
library(fastICA)
library(cluster)    # clustering algorithms
# install.packages("factoextra")
# install.packages("NbClust")
library(factoextra)
library(NbClust)
```

```{r}
# read in Auto data
Auto_data <- read_csv("/Users/ericachio/Documents/sackler/machine learning/hw1-ericachio/Auto.csv")

#remove cars with unknown horsepower and set horsepower to numeric
Auto_data <- Auto_data %>% 
  filter(horsepower != "?") %>% 
  mutate(horsepower = as.numeric(horsepower)) %>% 
  as.data.frame()

#save car names 
Auto_data_names <- Auto_data$name

#data to cluster
Auto_data_clust <- Auto_data[,1:8]
dim(Auto_data_clust)

#392 is too much for a demo, so lets take the first 25
Auto_data_clust <- Auto_data_clust[1:25,]
rownames(Auto_data_clust) <- Auto_data_names[1:25]
```


## Hierarchical agglomerative clustering

Step 1. Assign each item to it's own cluster. We start with 25 clusters, one for each car. 

Step 2. Calculate a proximity matrix between each cluster.

Step 3. Find the pair of clusters closest to each other. 

Step 4. Merge these clusters and then recalculate similarity between clusters. Some options are: single linkage (distance is calculated from the nearest neighbors), complete linkage (distance is calculated from furthest neighbor), average linkage (distance is calculated from mean of different clusters). 

Step 5. Repeat Step 3 and 4 until there is only one cluster.

### In practice

Step 1. Each car is a cluster. 

Step 2. Create a distance matrix from Auto_data_clust.

```{r}
help("dist")
hierarchical_dist <- as.matrix(dist(Auto_data_clust, method = "euclidean"))
#View(hierarchical_dist)
```

Step 3. Find the two cars that are the most similar to each other and print the names of those two cars

```{r }
diag(hierarchical_dist) <- NA
arrayInd(which.min(hierarchical_dist), dim(hierarchical_dist))

#postitions 23 and 15 are the most similar. Lets go back to the names of the cars
Auto_data_names[23]
Auto_data_names[15]

```

Step 4. Merge the two clusters together using average linkage. 

```{r }
#replace pos 15 with the average of pos 15 and 23
hierarchical_dist[,15] <- apply((hierarchical_dist[,c(23,15)]),1,mean)
hierarchical_dist[15,] <- apply((hierarchical_dist[c(23,15),]),2,mean)

#remove pos 23
hierarchical_dist <- hierarchical_dist[-23,-23]

#now position 15 represents the cluster containing the saab99e and the toyota corona mark ii
```

Step 5. To complete the algorithm, go back to step 3 and iterate through all of the previous steps until there are no more rows left

```{r }
diag(hierarchical_dist) <- NA
#all distances between x and x would be 0, so set to na because looking for min 
arrayInd(which.min(hierarchical_dist), dim(hierarchical_dist))

#postitions 4 and 3 are the most similar
Auto_data_names[4]
Auto_data_names[3]

```

### R function 

Now that we know how the algorithm works, let's use the R function hclust. Plot the Dendogram resulting from clustering the Auto_data_clust using average linkage.  

```{r}
hierarchical_dist <- dist(Auto_data_clust, method = "euclidean")
tree <- hclust(hierarchical_dist, method="average")
plot(tree)
```

There is one more element to hierarchical clustering: Cutting the tree. Here, we can control how many clusters we want or the height of the tree. 
```{r}
#help(cutree)

# cut tree into 3 clusters
tree <- hclust(hierarchical_dist, method="average")
plot(tree)
tree_k2 <- cutree(tree, k = 2)
# plot the tree before running this line 
rect.hclust(tree, k = 3, h = NULL)
```


\newpage

## Principal Components Analysis (PCA)

Principal Components Analysis is a linear dimensionality reduction algorithm. If you want to learn more about linear algebra, I suggest the MIT Open Courseware class here : https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/
There are two ways of doing PCA, Single Value Decomposition (SVD), and the method we will use today, using the covariance matrix of the data. 


Step 1. Center data by subtracting the mean.

Step 2. Calculate covariance matrix of data.

Step 3. Perform Eigendecomposition of the covariance matrix. i.e. represent the matrix in terms of it's eigenvalues and eigen vectors

Step 4. Multiply the eigen vectors by the original data to express the data in terms of the eigen vectors. 

Step 1. Center the data by subtracting the mean of the each column from the values in that column

```{r}
Auto_data_clust_pca <- data.matrix(Auto_data_clust)

Center_auto <- apply(Auto_data_clust_pca, 2, function(x) x - mean(x))
```


Step 2. Calculate covariance matrix of the Auto data
-result in a square matrix
-doing it by hand must be in a square matrix

```{r}

Covariance_auto <- cov(Center_auto)
```

Step 3.  Calculate eigen values and vectors
-eigenvectors the axis of the highest variation 

```{r}
Eigen_value_auto <- eigen(Covariance_auto)$value

#columns are the eigen vectors
Eigen_vector_auto <- eigen(Covariance_auto)$vector
```

Step 4. Multiply the eigen vector matrix by the original data. 
-changing orientation of data by multiplying by eigen vector (8x8?)
-changing 8 values that equate to cars 
-now that we shifted, plot to ggplot

```{r}
PC <- as.data.frame(data.matrix(Center_auto) %*% Eigen_vector_auto)

ggplot(PC, aes(PC[,1], PC[,2])) + geom_point(aes(PC[,1], PC[,2]))
#+ geom_text(aes(label=Auto_data_names[1:8]), nudge_x = -2.5, nudge_y = 400)
```

Step 5. Find out which principal components explain the variance in the data. 
-changing the way data is represented to show maximum variation

```{r}
#for each component, take the cumulative sum of eigen values up to that point and and divide by the total sum of eigen values
round(cumsum(Eigen_value_auto)/sum(Eigen_value_auto) * 100, digits = 2)
```

Principal component 1 and 2 explain 99.99 percent of the variance. Principal component 1,2, and 3 together explain 100% of the variance in the data. 

### R function 
Now that we know how PCA works, lets use the R funtion prcomp.

```{r}
help("prcomp")
autoplot(prcomp(Auto_data_clust_pca))
```

\newpage

## Independent Component Analysis (ICA)
ICA is an algorithm that finds components that are independent, subcomponents of the data. 

-trying to split x amount of signals apart

Step 1. Whiten the data by projecting the data onto the eigen vectors (PCA).

Step 2. Solve the X=AS equation by maximizing non-gaussianty in the variables(components) in S. 

This results in a matrix S with components that are independent from each other. 

We will use the fastICA algorithm.

First we will go backwards. 
Create a matrix S with the independent components
```{r}
#create two signals
S <- cbind(cos((1:500)/10), ((500:1)/1000))

par(mfcol = c(1, 2))
plot(S[,1], type="l")
plot(S[,2], type="l")
```

Create a mixing matrix A
```{r}
A <- matrix(c(0.5, 0.7, 0.423, 0.857), 2, 2)
```

Mix S using A
```{r}
X <- S %*% A
par(mfcol = c(1, 2))
plot(X[,1], type="l")
plot(X[,2], type="l")

```

Unmix using fastICA
```{r, include=FALSE}
a <- fastICA(X, 2, alg.typ = "parallel", fun = "logcosh", alpha = 1,
             method = "R", row.norm = FALSE, maxit = 200,
             tol = 0.0001, verbose = TRUE)
```


```{r}
par(mfcol = c(1, 2))
plot(1:500, a$S[,1], type = "l", xlab = "S'1", ylab = "")
plot(1:500, a$S[,2], type = "l", xlab = "S'2", ylab = "")
```


### ICA on the auto data
-8 features, so no more than 7 (param)
-pick right number of signals

-genesample - dont look for more modules than u have samples 
```{r, include=FALSE}
a <- fastICA(Auto_data_clust, 7, alg.typ = "parallel", fun = "logcosh", alpha = 1,
             method = "R", row.norm = FALSE, maxit = 200,
             tol = 0.0001, verbose = TRUE)
```

plot the independent components as a heatmap
```{r}
heatmap(a$S)
```


\newpage

## Homework

```{r}
data(iris)

```

0. Subset the Iris dataset to only include `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`. 
```{r}
subsetIris <- iris[,c(1,2,3,4)]
subsetIris
```


1. Write out the Kmeans algorithm by hand, and run two iterations of it. 
```{r}
#to do
#choosing k number clusters (2)
#split randomly 
#assign each flower to cluster, where each flower is in a cluster

#subset each cluster out
#get centroid of cluster - average of all flower attributes 
# distance of each flower to its clusters centroid - > hypo
#(f1x - cx)^2 + (f1y - cy)^2 + petallength/width
#distance from f1 to other centroids 
#min distance between centroids - which centroid it belongs to?
#reassign clusters. 
```

```{r}
newData = cbind(subsetIris, sample(c(0,1), size = nrow(subsetIris), replace = TRUE))
names(newData) <- c("Sepal.Length","Sepal.Width", "Petal.Length","Petal.Width","Cluster")
newData
```

```{r}
#subset into two clusters:
cluster0 <- filter(newData, Cluster == 0)
cluster0
cluster1 <- filter(newData, Cluster == 1)
cluster1
```

```{r}
#average centroid:
a <- summarise(cluster0, mean_sepallength = mean(Sepal.Length))
b <- summarise(cluster0, mean_sepalwidth = mean(Sepal.Width))
c <- summarise(cluster0, mean_sepallength = mean(Petal.Length))
d <- summarise(cluster0, mean_sepalwidth = mean(Petal.Width))

averageCluster0 <- c(a,b,c,d)
averageCluster0

e <- summarise(cluster1, mean_sepallength = mean(Sepal.Length))
f <- summarise(cluster1, mean_sepalwidth = mean(Sepal.Width))
g <- summarise(cluster1, mean_sepallength = mean(Petal.Length))
h <- summarise(cluster1, mean_sepalwidth = mean(Petal.Width))

averageCluster1 <- c(e,f,g,h)
# averageCluster1
```

```{r}
#get distance of each flower to average centroid
for(i in 1:nrow(cluster0)) {
  cluster0[i,6:9] <- cluster0[i,1:4] - averageCluster0
  cluster0[i,6:9] <- (cluster0[i,6:9] ^ 2)
  cluster0[i,10:13] <- cluster0[i,1:4] - averageCluster1
  cluster0[i,10:13] <- (cluster0[i,10:13] ^ 2)
}
cluster0 <- transform(cluster0, sum0=rowSums(cluster0[,6:9]))
cluster0 <- transform(cluster0, sum1=rowSums(cluster0[,10:13]))
cluster0


for(i in 1:nrow(cluster1)) {
  cluster1[i,6:9] <- cluster1[i,1:4] - averageCluster0
  cluster1[i,6:9] <- (cluster1[i,6:9] ^ 2)
  cluster1[i,10:13] <- cluster1[i,1:4] - averageCluster1
  cluster1[i,10:13] <- (cluster1[i,10:13] ^ 2)
}
cluster1 <- transform(cluster1, sum0=rowSums(cluster1[,6:9]))
cluster1 <- transform(cluster1, sum1=rowSums(cluster1[,10:13]))
cluster1
```
```{r}
#recluster
for(i in 1:nrow(cluster0)) {
  if (cluster0[i,14] < cluster0[i,15]){
    cluster0[i,5] = 0
  }else{
    cluster0[i,5] = 1
  }
}
# cluster0
temp <- filter(cluster0, Cluster == 0)
temp2 <- filter(cluster0, Cluster == 1)
temp
temp2
for(i in 1:nrow(cluster1)) {
  if (cluster1[i,14] < cluster1[i,15]){
    cluster1[i,5] = 0
  }else{
    cluster1[i,5] = 1
  }
}
# cluster1
temp3 <- filter(cluster1, Cluster == 0)
temp4 <- filter(cluster1, Cluster == 1)
temp3 #57
temp4 #93
```

```{r}
#merged together to separate into correct cluster vectors again
fullIris <- rbind(cluster0, cluster1)
fullIris
cluster0 <- filter(fullIris, Cluster == 0)
cluster0
cluster1 <- filter(fullIris, Cluster == 1)
cluster1
```

```{r}
#2nd iteration
#average centroid:
a <- summarise(cluster0, mean_sepallength = mean(Sepal.Length))
b <- summarise(cluster0, mean_sepalwidth = mean(Sepal.Width))
c <- summarise(cluster0, mean_sepallength = mean(Petal.Length))
d <- summarise(cluster0, mean_sepalwidth = mean(Petal.Width))

averageCluster0 <- c(a,b,c,d)
# averageCluster0

e <- summarise(cluster1, mean_sepallength = mean(Sepal.Length))
f <- summarise(cluster1, mean_sepalwidth = mean(Sepal.Width))
g <- summarise(cluster1, mean_sepallength = mean(Petal.Length))
h <- summarise(cluster1, mean_sepalwidth = mean(Petal.Width))

averageCluster1 <- c(e,f,g,h)
# averageCluster1

#get distance of each flower to average centroid
for(i in 1:nrow(cluster0)) {
  cluster0[i,6:9] <- cluster0[i,1:4] - averageCluster0
  cluster0[i,6:9] <- (cluster0[i,6:9] ^ 2)
  cluster0[i,10:13] <- cluster0[i,1:4] - averageCluster1
  cluster0[i,10:13] <- (cluster0[i,10:13] ^ 2)
}
cluster0 <- transform(cluster0, sum0=rowSums(cluster0[,6:9]))
cluster0 <- transform(cluster0, sum1=rowSums(cluster0[,10:13]))
# cluster0

for(i in 1:nrow(cluster1)) {
  cluster1[i,6:9] <- cluster1[i,1:4] - averageCluster0
  cluster1[i,6:9] <- (cluster1[i,6:9] ^ 2)
  cluster1[i,10:13] <- cluster1[i,1:4] - averageCluster1
  cluster1[i,10:13] <- (cluster1[i,10:13] ^ 2)
}
cluster1 <- transform(cluster1, sum0=rowSums(cluster1[,6:9]))
cluster1 <- transform(cluster1, sum1=rowSums(cluster1[,10:13]))
# cluster1

#re-cluster
for(i in 1:nrow(cluster0)) {
  if (cluster0[i,14] < cluster0[i,15]){
    cluster0[i,5] = 0
  }else{
    cluster0[i,5] = 1
  }
}
# cluster0
for(i in 1:nrow(cluster1)) {
  if (cluster1[i,14] < cluster1[i,15]){
    cluster1[i,5] = 0
  }else{
    cluster1[i,5] = 1
  }
}
# cluster1

#combining singular clusters back to 1 dataframe 
fullIris <- rbind(cluster0, cluster1)
# fullIris

#split into 2 vectors 
cluster0 <- filter(fullIris, Cluster == 0)
cluster0
cluster1 <- filter(fullIris, Cluster == 1)
cluster1

```

2. Run PCA on the Iris dataset. Plot a scatter plot of PC1 vs PC2 and include the percent variance those PCs describe. 
```{r}
help("prcomp")
iris_pca <- data.matrix(subsetIris)
autoplot(prcomp(iris_pca))
```


3. Run ICA on the Iris dataset. Plot the independent components as a heatmap.
```{r}
help("fastICA")
a <- fastICA(subsetIris, 4, alg.typ = "parallel", fun = "logcosh", alpha = 1,
             method = "R", row.norm = FALSE, maxit = 200,
             tol = 0.0001, verbose = TRUE)
heatmap(a$S)
```


4. Use Kmeans to cluster the Iris data. 
  * Use the silhouette function in the cluster package to find the optimal number of clusters for kmeans for the iris dataset. Then cluster using kmeans clustering. Does the data cluster by species? 
  * Using this clustering, color the PCA plot according to the clusters.

```{r}
fviz_nbclust(subsetIris, kmeans, method = "silhouette")
kIris <- kmeans(subsetIris, 2)
# kIris
# kiris_pca <- data.matrix(kIris)
```

```{r}
autoplot(prcomp(iris_pca), col=kIris$cluster)
```

  
5. Use hierarchical clustering to cluster the Iris data.

  * Try two different linkage types, and two different distance metrics. (euclidean/manhattan) 
  * For one linkage type and one distance metric, try two different cut points.                       
  * Using this clustering, color the PCA plot according to the clusters. (6  plots in total)
  
- 6 clusters 
- cut point by cut or height
- color PCA by making column for cut point clusters
  
```{r}
#two different linkage
#single
hierarchical_dist <- dist(subsetIris, method = "euclidean")
tree <- hclust(hierarchical_dist, method="single")
plot(tree)

#complete
hierarchical_dist <- dist(subsetIris, method = "euclidean")
tree <- hclust(hierarchical_dist, method="complete")
plot(tree)

#two different distance metrics
#manhattan
hierarchical_dist <- dist(subsetIris, method = "manhattan")
tree <- hclust(hierarchical_dist, method="average")
plot(tree)

#minkowski 
hierarchical_dist <- dist(subsetIris, method = "minkowski")
tree <- hclust(hierarchical_dist, method="average")
plot(tree)
```
```{r}
#single
hierarchical_dist <- dist(subsetIris, method = "euclidean")
tree <- hclust(hierarchical_dist, method="single")
plot(tree)
tree_k1 <- cutree(tree, k = 2)
rect.hclust(tree, k = 2, h = NULL)
firstCutTree <- cbind(subsetIris, tree_k1)
# firstCutTree
autoplot(prcomp(iris_pca), col=firstCutTree$tree_k1)

#manhattan
hierarchical_dist <- dist(subsetIris, method = "manhattan")
tree <- hclust(hierarchical_dist, method="average")
plot(tree)
tree_k2 <- cutree(tree, k = 4)
rect.hclust(tree, k = 4, h = NULL)
# tree_k2
secondCutTree <- cbind(subsetIris, tree_k2)
# secondCutTree
autoplot(prcomp(iris_pca), col=secondCutTree$tree_k2)
```


# Optional material
On PCA:

Eigen Vectors and Eigen Values http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/
Linear Algebra by Prof. Gilbert Strang https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/
http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf
https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues

On ICA: 

Independent Component Analysis: Algorithms and Applications https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf
Tutorial on ICA taken from http://rstudio-pubs-static.s3.amazonaws.com/93614_be30df613b2a4707b3e5a1a62f631d19.html



TA notes:
-generally normalzied.
